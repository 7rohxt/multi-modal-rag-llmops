{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58ea40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run 06_caching.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "171a5780",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b58ee8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    temperature=0,\n",
    "    openai_api_key=os.getenv(\"MY_OPENAI_API_KEY\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd902045",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    MessagesPlaceholder(\"history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8529824",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e22639d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "store = {}  # can hold multiple sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5d6022a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_memory = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    lambda session_id: store.setdefault(session_id, InMemoryChatMessageHistory()),\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"history\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bbadee2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def memory_chat(query, session_id=\"session_1\"):\n",
    "    result = with_memory.invoke(\n",
    "        {\"input\": query},\n",
    "        config={\"configurable\": {\"session_id\": session_id}}\n",
    "    )\n",
    "    return result.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0b006342",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_query(query, session_id=\"session_1\"):\n",
    "\n",
    "    # STEP 0 — ROUTER\n",
    "    decision = route_query(query)\n",
    "    print(\"Router decision:\", decision)\n",
    "\n",
    "    # ---------------------------\n",
    "    # DIRECT MODE → use MEMORY\n",
    "    # ---------------------------\n",
    "    if decision == \"direct\":\n",
    "        answer = memory_chat(query, session_id=session_id)\n",
    "        cache_set(query, answer)\n",
    "        return answer\n",
    "\n",
    "    # ---------------------------\n",
    "    # RAG MODE\n",
    "    # ---------------------------\n",
    "\n",
    "    # STEP 1 — CACHE CHECK\n",
    "    cached = cache_get(query)\n",
    "    if cached:\n",
    "        print(\"CACHE HIT\")\n",
    "        return cached\n",
    "\n",
    "    print(\"CACHE MISS → Running full RAG pipeline...\")\n",
    "\n",
    "    # STEP 2 — RETRIEVE\n",
    "    retrieved = retrieve_candidates(query)\n",
    "\n",
    "    # STEP 3 — RERANK\n",
    "    reranked = rerank_documents(query, retrieved, top_k=6)\n",
    "\n",
    "    # STEP 4 — GENERATE\n",
    "    answer = generate_answer(query, reranked)\n",
    "\n",
    "    # STEP 5 — STORE FINAL ANSWER IN CACHE\n",
    "    cache_set(query, answer)\n",
    "\n",
    "    # STEP 6 — STORE BOTH QUERY & ANSWER IN MEMORY\n",
    "    with_memory.invoke(\n",
    "        {\"input\": f\"USER: {query}\"},\n",
    "        config={\"configurable\": {\"session_id\": session_id}}\n",
    "    )\n",
    "\n",
    "    with_memory.invoke(\n",
    "        {\"input\": f\"ASSISTANT: {answer}\"},\n",
    "        config={\"configurable\": {\"session_id\": session_id}}\n",
    "    )\n",
    "\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "11276991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Router decision: rag\n",
      "CACHE MISS → Running full RAG pipeline...\n",
      "BM25 retrieved: 10 chunks\n",
      "Semantic retrieved: 10 chunks\n",
      "Combined unique: 17\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Amazon reported that AWS (Amazon Web Services) revenue increased 19% year-over-year in 2024, from $91 billion to $108 billion. For perspective, just 10 years ago, AWS revenue was $4.6 billion.'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_query(\"What did Amazon report about cloud revenue in 2024?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fbd345fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Router decision: direct\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Yes, you asked about Amazon earlier in this session. Specifically, you inquired about Amazon's overall revenue in 2024 and also about Amazon's cloud revenue (AWS) in 2024. If you have any more questions or need further information, feel free to ask!\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_query(\"did i ask about Amazon earlier?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multi-modal-rag-llmops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
