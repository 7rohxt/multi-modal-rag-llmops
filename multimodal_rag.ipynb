{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "10924d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.documents import Document\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import torch\n",
    "load_dotenv()\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import fitz \n",
    "import io\n",
    "import base64\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from langchain_community.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b780d4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4.1\",\n",
    "    temperature=0,\n",
    "    openai_api_key=os.getenv(\"MY_OPENAI_API_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e15240",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_model=CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_processor=CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3061b673",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbfdcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_image(image_data):\n",
    "    \"\"\"Embed image using CLIP.\"\"\"\n",
    "    \n",
    "    # Load from path or PIL object\n",
    "    if isinstance(image_data, str):\n",
    "        image = Image.open(image_data).convert(\"RGB\")\n",
    "    else:\n",
    "        image = image_data.convert(\"RGB\")\n",
    "    \n",
    "    # Process image\n",
    "    inputs = clip_processor(images=image, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        features = clip_model.get_image_features(**inputs)\n",
    "    \n",
    "    # Normalize to unit vector\n",
    "    features = features / features.norm(dim=-1, keepdim=True)\n",
    "    return features.squeeze().cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a90d1d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_text(text):\n",
    "    \"\"\"Embed text using CLIP.\"\"\"\n",
    "    \n",
    "    inputs = clip_processor(\n",
    "        text=text,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=77\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        features = clip_model.get_text_features(**inputs)\n",
    "    \n",
    "    # Normalize\n",
    "    features = features / features.norm(dim=-1, keepdim=True)\n",
    "    return features.squeeze().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a70d6153",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Process PDF\n",
    "pdf_path=\"data/multimodal_sample.pdf\"\n",
    "doc=fitz.open(pdf_path)\n",
    "# Storage for all documents and embeddings\n",
    "all_docs = []\n",
    "all_embeddings = []\n",
    "image_data_store = {}  # Store actual image data for LLM\n",
    "\n",
    "# Text splitter\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ec87c936",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,page in enumerate(doc):\n",
    "    ## process text\n",
    "    text=page.get_text()\n",
    "    if text.strip():\n",
    "        ##create temporary document for splitting\n",
    "        temp_doc = Document(page_content=text, metadata={\"page\": i, \"type\": \"text\"})\n",
    "        text_chunks = splitter.split_documents([temp_doc])\n",
    "\n",
    "        #Embed each chunk using CLIP\n",
    "        for chunk in text_chunks:\n",
    "            embedding = embed_text(chunk.page_content)\n",
    "            all_embeddings.append(embedding)\n",
    "            all_docs.append(chunk)\n",
    "\n",
    "\n",
    "\n",
    "    ## process images\n",
    "    ##Three Important Actions:\n",
    "\n",
    "    ##Convert PDF image to PIL format\n",
    "    ##Store as base64 for GPT-4V (which needs base64 images)\n",
    "    ##Create CLIP embedding for retrieval\n",
    "\n",
    "    for img_index, img in enumerate(page.get_images(full=True)):\n",
    "        try:\n",
    "            xref = img[0]\n",
    "            base_image = doc.extract_image(xref)\n",
    "            image_bytes = base_image[\"image\"]\n",
    "            \n",
    "            # Convert to PIL Image\n",
    "            pil_image = Image.open(io.BytesIO(image_bytes)).convert(\"RGB\")\n",
    "            \n",
    "            # Create unique identifier\n",
    "            image_id = f\"page_{i}_img_{img_index}\"\n",
    "            \n",
    "            # Store image as base64 for later use with GPT-4V\n",
    "            buffered = io.BytesIO()\n",
    "            pil_image.save(buffered, format=\"PNG\")\n",
    "            img_base64 = base64.b64encode(buffered.getvalue()).decode()\n",
    "            image_data_store[image_id] = img_base64\n",
    "            \n",
    "            # Embed image using CLIP\n",
    "            embedding = embed_image(pil_image)\n",
    "            all_embeddings.append(embedding)\n",
    "            \n",
    "            # Create document for image\n",
    "            image_doc = Document(\n",
    "                page_content=f\"[Image: {image_id}]\",\n",
    "                metadata={\"page\": i, \"type\": \"image\", \"image_id\": image_id}\n",
    "            )\n",
    "            all_docs.append(image_doc)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {img_index} on page {i}: {e}\")\n",
    "            continue\n",
    "\n",
    "doc.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e692909",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'page': 0, 'type': 'text'}, page_content='Annual Revenue Overview\\nThis document summarizes the revenue trends across Q1, Q2, and Q3. As illustrated in the chart\\nbelow, revenue grew steadily with the highest growth recorded in Q3.\\nQ1 showed a moderate increase in revenue as new product lines were introduced. Q2 outperformed\\nQ1 due to marketing campaigns. Q3 had exponential growth due to global expansion.'),\n",
       " Document(metadata={'page': 0, 'type': 'image', 'image_id': 'page_0_img_0'}, page_content='[Image: page_0_img_0]')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a818239b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00267245,  0.01282999, -0.05183142, ..., -0.00385085,\n",
       "         0.02977721, -0.00010685],\n",
       "       [ 0.01732339, -0.01327691, -0.02427031, ...,  0.08994055,\n",
       "        -0.00272157,  0.03253039]], shape=(2, 512), dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_array = np.array(all_embeddings)\n",
    "embeddings_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6d0816fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([Document(metadata={'page': 0, 'type': 'text'}, page_content='Annual Revenue Overview\\nThis document summarizes the revenue trends across Q1, Q2, and Q3. As illustrated in the chart\\nbelow, revenue grew steadily with the highest growth recorded in Q3.\\nQ1 showed a moderate increase in revenue as new product lines were introduced. Q2 outperformed\\nQ1 due to marketing campaigns. Q3 had exponential growth due to global expansion.'),\n",
       "  Document(metadata={'page': 0, 'type': 'image', 'image_id': 'page_0_img_0'}, page_content='[Image: page_0_img_0]')],\n",
       " array([[-0.00267245,  0.01282999, -0.05183142, ..., -0.00385085,\n",
       "          0.02977721, -0.00010685],\n",
       "        [ 0.01732339, -0.01327691, -0.02427031, ...,  0.08994055,\n",
       "         -0.00272157,  0.03253039]], shape=(2, 512), dtype=float32))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(all_docs,embeddings_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b41d8faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`embedding_function` is expected to be an Embeddings object, support for passing in a function will soon be removed.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x1f6489431c0>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create custom FAISS index since we have precomputed embeddings\n",
    "vector_store = FAISS.from_embeddings(\n",
    "    text_embeddings=[(doc.page_content, emb) for doc, emb in zip(all_docs, embeddings_array)],\n",
    "    embedding=None,  # We're using precomputed embeddings\n",
    "    metadatas=[doc.metadata for doc in all_docs]\n",
    ")\n",
    "vector_store"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multi-modal-rag-llmops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
